{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-23T08:41:42.004148Z",
     "iopub.status.busy": "2025-06-23T08:41:42.003883Z",
     "iopub.status.idle": "2025-06-23T08:43:00.263826Z",
     "shell.execute_reply": "2025-06-23T08:43:00.263099Z",
     "shell.execute_reply.started": "2025-06-23T08:41:42.004104Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/input/othersnotpy/requirments.txt (line 2)) (4.51.3)\n",
      "Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/input/othersnotpy/requirments.txt (line 3)) (0.14.0)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/input/othersnotpy/requirments.txt (line 4)) (1.5.2)\n",
      "Collecting bitsandbytes (from -r /kaggle/input/othersnotpy/requirments.txt (line 5))\n",
      "  Downloading bitsandbytes-0.46.0-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/input/othersnotpy/requirments.txt (line 6)) (3.4.1)\n",
      "Collecting faiss-cpu (from -r /kaggle/input/othersnotpy/requirments.txt (line 7))\n",
      "  Downloading faiss_cpu-1.11.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: flask in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/input/othersnotpy/requirments.txt (line 10)) (3.1.0)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/input/othersnotpy/requirments.txt (line 13)) (3.6.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/input/othersnotpy/requirments.txt (line 16)) (4.67.1)\n",
      "Requirement already satisfied: torch>=2.0 in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/input/othersnotpy/requirments.txt (line 19)) (2.6.0+cu124)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers>=4.38.0->-r /kaggle/input/othersnotpy/requirments.txt (line 2)) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.38.0->-r /kaggle/input/othersnotpy/requirments.txt (line 2)) (0.31.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.38.0->-r /kaggle/input/othersnotpy/requirments.txt (line 2)) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.38.0->-r /kaggle/input/othersnotpy/requirments.txt (line 2)) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.38.0->-r /kaggle/input/othersnotpy/requirments.txt (line 2)) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.38.0->-r /kaggle/input/othersnotpy/requirments.txt (line 2)) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers>=4.38.0->-r /kaggle/input/othersnotpy/requirments.txt (line 2)) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.38.0->-r /kaggle/input/othersnotpy/requirments.txt (line 2)) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.38.0->-r /kaggle/input/othersnotpy/requirments.txt (line 2)) (0.5.3)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft->-r /kaggle/input/othersnotpy/requirments.txt (line 3)) (7.0.0)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers->-r /kaggle/input/othersnotpy/requirments.txt (line 6)) (1.2.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers->-r /kaggle/input/othersnotpy/requirments.txt (line 6)) (1.15.2)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers->-r /kaggle/input/othersnotpy/requirments.txt (line 6)) (11.1.0)\n",
      "Requirement already satisfied: Werkzeug>=3.1 in /usr/local/lib/python3.11/dist-packages (from flask->-r /kaggle/input/othersnotpy/requirments.txt (line 10)) (3.1.3)\n",
      "Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from flask->-r /kaggle/input/othersnotpy/requirments.txt (line 10)) (3.1.6)\n",
      "Requirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.11/dist-packages (from flask->-r /kaggle/input/othersnotpy/requirments.txt (line 10)) (2.2.0)\n",
      "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from flask->-r /kaggle/input/othersnotpy/requirments.txt (line 10)) (8.1.8)\n",
      "Requirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.11/dist-packages (from flask->-r /kaggle/input/othersnotpy/requirments.txt (line 10)) (1.9.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets->-r /kaggle/input/othersnotpy/requirments.txt (line 13)) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets->-r /kaggle/input/othersnotpy/requirments.txt (line 13)) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets->-r /kaggle/input/othersnotpy/requirments.txt (line 13)) (2.2.3)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets->-r /kaggle/input/othersnotpy/requirments.txt (line 13)) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets->-r /kaggle/input/othersnotpy/requirments.txt (line 13)) (0.70.16)\n",
      "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r /kaggle/input/othersnotpy/requirments.txt (line 13))\n",
      "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->-r /kaggle/input/othersnotpy/requirments.txt (line 19)) (4.13.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->-r /kaggle/input/othersnotpy/requirments.txt (line 19)) (3.4.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->-r /kaggle/input/othersnotpy/requirments.txt (line 19)) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->-r /kaggle/input/othersnotpy/requirments.txt (line 19)) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->-r /kaggle/input/othersnotpy/requirments.txt (line 19)) (12.4.127)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0->-r /kaggle/input/othersnotpy/requirments.txt (line 19))\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0->-r /kaggle/input/othersnotpy/requirments.txt (line 19))\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0->-r /kaggle/input/othersnotpy/requirments.txt (line 19))\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0->-r /kaggle/input/othersnotpy/requirments.txt (line 19))\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0->-r /kaggle/input/othersnotpy/requirments.txt (line 19))\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0->-r /kaggle/input/othersnotpy/requirments.txt (line 19))\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->-r /kaggle/input/othersnotpy/requirments.txt (line 19)) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->-r /kaggle/input/othersnotpy/requirments.txt (line 19)) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->-r /kaggle/input/othersnotpy/requirments.txt (line 19)) (12.4.127)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0->-r /kaggle/input/othersnotpy/requirments.txt (line 19))\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->-r /kaggle/input/othersnotpy/requirments.txt (line 19)) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->-r /kaggle/input/othersnotpy/requirments.txt (line 19)) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0->-r /kaggle/input/othersnotpy/requirments.txt (line 19)) (1.3.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r /kaggle/input/othersnotpy/requirments.txt (line 13)) (3.11.18)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers>=4.38.0->-r /kaggle/input/othersnotpy/requirments.txt (line 2)) (1.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2>=3.1.2->flask->-r /kaggle/input/othersnotpy/requirments.txt (line 10)) (3.0.2)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers>=4.38.0->-r /kaggle/input/othersnotpy/requirments.txt (line 2)) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers>=4.38.0->-r /kaggle/input/othersnotpy/requirments.txt (line 2)) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers>=4.38.0->-r /kaggle/input/othersnotpy/requirments.txt (line 2)) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers>=4.38.0->-r /kaggle/input/othersnotpy/requirments.txt (line 2)) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers>=4.38.0->-r /kaggle/input/othersnotpy/requirments.txt (line 2)) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers>=4.38.0->-r /kaggle/input/othersnotpy/requirments.txt (line 2)) (2.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.38.0->-r /kaggle/input/othersnotpy/requirments.txt (line 2)) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.38.0->-r /kaggle/input/othersnotpy/requirments.txt (line 2)) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.38.0->-r /kaggle/input/othersnotpy/requirments.txt (line 2)) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.38.0->-r /kaggle/input/othersnotpy/requirments.txt (line 2)) (2025.4.26)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->-r /kaggle/input/othersnotpy/requirments.txt (line 13)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->-r /kaggle/input/othersnotpy/requirments.txt (line 13)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->-r /kaggle/input/othersnotpy/requirments.txt (line 13)) (2025.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers->-r /kaggle/input/othersnotpy/requirments.txt (line 6)) (1.5.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers->-r /kaggle/input/othersnotpy/requirments.txt (line 6)) (3.6.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r /kaggle/input/othersnotpy/requirments.txt (line 13)) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r /kaggle/input/othersnotpy/requirments.txt (line 13)) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r /kaggle/input/othersnotpy/requirments.txt (line 13)) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r /kaggle/input/othersnotpy/requirments.txt (line 13)) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r /kaggle/input/othersnotpy/requirments.txt (line 13)) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r /kaggle/input/othersnotpy/requirments.txt (line 13)) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r /kaggle/input/othersnotpy/requirments.txt (line 13)) (1.20.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets->-r /kaggle/input/othersnotpy/requirments.txt (line 13)) (1.17.0)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers>=4.38.0->-r /kaggle/input/othersnotpy/requirments.txt (line 2)) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers>=4.38.0->-r /kaggle/input/othersnotpy/requirments.txt (line 2)) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers>=4.38.0->-r /kaggle/input/othersnotpy/requirments.txt (line 2)) (1.3.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers>=4.38.0->-r /kaggle/input/othersnotpy/requirments.txt (line 2)) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers>=4.38.0->-r /kaggle/input/othersnotpy/requirments.txt (line 2)) (2024.2.0)\n",
      "Downloading bitsandbytes-0.46.0-py3-none-manylinux_2_24_x86_64.whl (67.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 MB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading faiss_cpu-1.11.0-cp311-cp311-manylinux_2_28_x86_64.whl (31.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m57.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m71.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, fsspec, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, faiss-cpu, bitsandbytes\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.9.41\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.9.41:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.9.41\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.10.19\n",
      "    Uninstalling nvidia-curand-cu12-10.3.10.19:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.10.19\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.4.0.6\n",
      "    Uninstalling nvidia-cufft-cu12-11.4.0.6:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.4.0.6\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.9.0.13\n",
      "    Uninstalling nvidia-cublas-cu12-12.9.0.13:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.9.0.13\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2025.3.2\n",
      "    Uninstalling fsspec-2025.3.2:\n",
      "      Successfully uninstalled fsspec-2025.3.2\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.9.5\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.9.5:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.9.5\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.7.4.40\n",
      "    Uninstalling nvidia-cusolver-cu12-11.7.4.40:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.7.4.40\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "cesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "bigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\n",
      "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed bitsandbytes-0.46.0 faiss-cpu-1.11.0 fsspec-2025.3.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
     ]
    }
   ],
   "source": [
    "!pip install requirments.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-23T08:43:17.589854Z",
     "iopub.status.busy": "2025-06-23T08:43:17.589589Z",
     "iopub.status.idle": "2025-06-23T08:43:43.734462Z",
     "shell.execute_reply": "2025-06-23T08:43:43.733725Z",
     "shell.execute_reply.started": "2025-06-23T08:43:17.589824Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m375.8/375.8 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.5/10.5 MB\u001b[0m \u001b[31m89.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m365.3/365.3 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for peft (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.31.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2025.3.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (6.0.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.13.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (1.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2025.4.26)\n"
     ]
    }
   ],
   "source": [
    "#all installs\n",
    "!pip install -q -U trl transformers accelerate git+https://github.com/huggingface/peft.git\n",
    "!pip install -q datasets bitsandbytes einops wandb\n",
    "!pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-23T08:43:43.736214Z",
     "iopub.status.busy": "2025-06-23T08:43:43.735862Z",
     "iopub.status.idle": "2025-06-23T08:44:08.164724Z",
     "shell.execute_reply": "2025-06-23T08:44:08.164158Z",
     "shell.execute_reply.started": "2025-06-23T08:43:43.736187Z"
    },
    "id": "ApQu4PT9VIvC"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-23 08:43:51.466759: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1750668231.628986      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1750668231.674674      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "from datasets import load_dataset, Dataset\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    GenerationConfig,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    Trainer,\n",
    "    AutoConfig\n",
    ")\n",
    "\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    "    PeftConfig,\n",
    "    PeftModel\n",
    ")\n",
    "\n",
    "from accelerate import init_empty_weights, load_checkpoint_and_dispatch\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import faiss\n",
    "import termios\n",
    "import tty\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Environment Variable Pytorch uses to tune its CUDA memory allocator(useful for LLMs)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-23T08:48:34.406091Z",
     "iopub.status.busy": "2025-06-23T08:48:34.405803Z",
     "iopub.status.idle": "2025-06-23T08:48:34.410684Z",
     "shell.execute_reply": "2025-06-23T08:48:34.409920Z",
     "shell.execute_reply.started": "2025-06-23T08:48:34.406070Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-23T08:48:44.299072Z",
     "iopub.status.busy": "2025-06-23T08:48:44.298676Z",
     "iopub.status.idle": "2025-06-23T08:48:44.308913Z",
     "shell.execute_reply": "2025-06-23T08:48:44.307975Z",
     "shell.execute_reply.started": "2025-06-23T08:48:44.299039Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-23T08:44:08.507280Z",
     "iopub.status.busy": "2025-06-23T08:44:08.506988Z",
     "iopub.status.idle": "2025-06-23T08:44:08.546918Z",
     "shell.execute_reply": "2025-06-23T08:44:08.546252Z",
     "shell.execute_reply.started": "2025-06-23T08:44:08.507256Z"
    },
    "id": "6AvaEgKC6hlS"
   },
   "outputs": [],
   "source": [
    "#Acceletrator\n",
    "from accelerate import FullyShardedDataParallelPlugin, Accelerator\n",
    "from torch.distributed.fsdp.fully_sharded_data_parallel import FullOptimStateDictConfig, FullStateDictConfig\n",
    "fsdp_plugin = FullyShardedDataParallelPlugin(\n",
    "    state_dict_config=FullStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
    "    optim_state_dict_config=FullOptimStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
    ")\n",
    "accelerator = Accelerator(fsdp_plugin=fsdp_plugin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hugging Face login to use Mistral7b(it's a gated repository)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5s3soyYM3gCv",
    "outputId": "dda4c976-7235-4347-a200-baa0e8b4e8eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
      "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
      "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
      "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
      "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
      "\n",
      "    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n",
      "    Setting a new token will erase the existing one.\n",
      "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
      "Enter your token (input will not be visible): \n",
      "Add token as git credential? (Y/n) n\n",
      "Token is valid (permission: fineGrained).\n",
      "The token `llm model` has been saved to /root/.cache/huggingface/stored_tokens\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful.\n",
      "The current active token is: `llm model`\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AFKogPbx3B0u"
   },
   "outputs": [],
   "source": [
    "# Load all JSON files into dataframes\n",
    "df1 = pd.read_json(\"/content/constitution_qa.json\")\n",
    "df2 = pd.read_json(\"/content/crpc_qa.json\")\n",
    "df3 = pd.read_json(\"/content/ipc_qa.json\")\n",
    "df4 = pd.read_json(\"/content/IndicLegalQA Dataset_10K_Revised.json\")\n",
    "\n",
    "# Combine all into a single dataframe\n",
    "combined_df = pd.concat([df1, df2, df3,df4], ignore_index=True)\n",
    "\n",
    "# Save to a new JSON file\n",
    "combined_df.to_json(\"combined.json\", orient=\"records\", indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LS0M9buB3t3C",
    "outputId": "3cb6ab70-cc1a-4802-9055-9d3d7031a503"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': '<s>[INST] What is India according to the Union and its Territory? [/INST] India, that is Bharat, shall be a Union of States.</s>'}\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "with open(\"combined.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)  # Ensure your JSON is a list of dicts, e.g., [{\"question\": \"...\", \"answer\": \"...\"}]\n",
    "\n",
    "# Format the data\n",
    "formatted_data = []\n",
    "for sample in data:\n",
    "    formatted_text = f\"<s>[INST] {sample['question']} [/INST] {sample['answer']}</s>\"\n",
    "    formatted_data.append({\"text\": formatted_text})\n",
    "\n",
    "# Convert to Hugging Face Dataset format\n",
    "dataset = Dataset.from_list(formatted_data)\n",
    "\n",
    "# Check formatted sample\n",
    "print(dataset[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "ef67894e568f49339a2adbd7ad57c46b",
      "e8d80a2e40b446d7870c928f8c9e48a2",
      "efd4fd8e21824c2cbbe4ba0e4450d300",
      "d0363fddf3b9496f82ff91309660bb5c",
      "1639021efb1f4b48869c7b3298da99c7",
      "55fdef117c0b4f0b9e814b871a221a07",
      "d2fc71aa07e44026b1c00eb55f62fe91",
      "12f9b301473c43bc9a1a50f4ea794431",
      "6013b5b8c3b842d6abcf87489dc3f22e",
      "ca45f2a4bb3c43649fe347654fcfc5f5",
      "5d72402b835e410ca9e84f6eeeb50319"
     ]
    },
    "id": "a8PMelW05GmE",
    "outputId": "e360379d-e207-435b-b351-87bf516f5fab"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef67894e568f49339a2adbd7ad57c46b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_path = \"combined.json\" \n",
    "\n",
    "data = load_dataset(\"json\", data_files=dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Specification and Bits and bytes config(very standard quantization )**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 367,
     "referenced_widgets": [
      "c591d86934bd45b08b20914609f32838",
      "d4ab170fdd5742f9aa72699b3408da6d",
      "84a165df0537427398c95d23645a4031",
      "549a0a6c88f743f08ad5cb161de89de5",
      "51d54dc2af294b9da954904ac87fca99",
      "7b22766e1e274ef8900e494297325f7a",
      "59a4752db0744f5e87e5ef0ba2381eb6",
      "50f5d2e3246242ba8a6d6092bb54e538",
      "580c5c2338e5466cb39e2e5caec44648",
      "5e31fbc8042d4a08beb5bd9ce6e938f3",
      "d95ff6c28e3e40688ffaa35325a709c1",
      "d3a2f8300ea048309c86dcba9026e71e",
      "07aa51c118654792b9c8a43a017a7054",
      "20ca47dbce3a4bbf952ade36914b886b",
      "625d63b375134e46a3ec2f035f06985e",
      "85b09d5876e143e1b8fa77596149ad4c",
      "c5297f05db5f4e39b3cbd9098e76efe7",
      "3fd70eb3b81d43009d52e16793ec419d",
      "0ee9f185990741088ed5fdf599690560",
      "917edf39d1d7442497407eca756bbcb2",
      "8dbbfcc3d4734e2da2a9ff28f65e7808",
      "d32ce57fdea3439cad116bc716908e34",
      "a896e5fb47ce44df998e41892968bb72",
      "f88118d6c9534706810fba04de352cb2",
      "f3e0e1cbdc8b42af9bfb835edac8b7d5",
      "f0055ef2f0e544f597ee4e7a9626ca0b",
      "0690f129515346148500464ae04c0cf3",
      "81f0f621a9fc46db8579ec1827c0037b",
      "686331d3c0ce41b79276d313ee17618a",
      "14b1ad8c58974eefa05ea478163b5b30",
      "d69cf6a4eed2400c993143e7061ebfe1",
      "45ac9b89058e48998c7288091954edf6",
      "54ca738e8dae42cab4f27798a9ac6376",
      "f46c4536719f4050ad2e0a6a3bdfad3a",
      "243ece62e05c4ad4a4ee2327eb29fb3f",
      "ff1a142dd9c3478aa2e46657a3399f10",
      "c05471f7177243168bb99684b892903f",
      "058a995df31d4efd8ec815a20a07c5d6",
      "c93efeeab6be472eaac45e844aac5f59",
      "4a18e4f2effa472aa1d04de1b80c2f8e",
      "6d3ce94892764a2298d26df488a847c9",
      "be8331153ff94c1895695b977346ac63",
      "850dd6ca6b67496fa31eb2bda47adab8",
      "a81ef98791fd456988c6412bdd60b948",
      "2ec42d2e3f754227a483677262f5c1cf",
      "641d33160ab84437bbabd3e719ea38f3",
      "6d7fa18f9f294032a55a849f2e5bb2c0",
      "29199d065be5475b9768edaaa9be37d3",
      "b23388b461f34f87b8cadd5ccadb6754",
      "bf0acb73c6004bfe97ed13ae70e8ae94",
      "8a8ed1f349584b5181ce27345e8c6d9d",
      "3e80ceaa57044ef0a7d4ec4d40d3311c",
      "713f14a1f91146a3b6dd3d854932be4b",
      "6f25db3e7a9745ab9425621cdfe8f870",
      "817302e5cf85416f8e25d38676f8978e",
      "8532d21fbff74e7fbb10c90d87e1c427",
      "14cb829d511142c995e796a962513f83",
      "09978ae2cbfa4557ab4e5c05bed52af9",
      "94032e7b9d59406e91a80f19ef6daeda",
      "34fb64cca80b4aa0b7d27c323ec2ddf0",
      "3f54eacc6ed640c3804e23a65308b7b0",
      "fd821586640a491e83fc4a55781a81a3",
      "e62b173e045e4c6d988383ffc0beb8c9",
      "8b2f096600d84c3fa9b8616809db1411",
      "6b6d08e5d77542cdb9c449b97ae4fcba",
      "0fe970b9fa3c4b98aa0528ffa4bb6e57",
      "4dabf22e307d4ed9b3c5c294d743adda",
      "460eed44a9d14454954a8b9376673ef4",
      "203c23a2dc9b44fcb4d81cea19cbcce2",
      "105947f4e3df46e08317e9f0facac20d",
      "ddde48d32942411ba6764ad8d7dcffd4",
      "46fe1a5f90e147679e0419c4fbdcdb78",
      "769f8a1e6e7845eeb1b7734cfdd3cdd0",
      "006c316268d34b7aad9825693e476d4c",
      "d1fcb262db1a4667afad55dce9a517a3",
      "ae3b9711f4f949cd9381d6d7fa97cc4c",
      "03c7c880a529485f8ce2b26c19d2fb78"
     ]
    },
    "id": "k1tgVoXr5qWe",
    "outputId": "a30cc8e2-62cc-417a-f8c1-dbb6a54d0d51"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c591d86934bd45b08b20914609f32838",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3a2f8300ea048309c86dcba9026e71e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a896e5fb47ce44df998e41892968bb72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f46c4536719f4050ad2e0a6a3bdfad3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ec42d2e3f754227a483677262f5c1cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8532d21fbff74e7fbb10c90d87e1c427",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dabf22e307d4ed9b3c5c294d743adda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tokenizer Initiation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145,
     "referenced_widgets": [
      "02ba0f5b2c78464eb883e42a8404328f",
      "1169537e32a54ff3bacd9512bfc30d23",
      "06835e29baa94219aa767b5a670bd148",
      "a1eb782aa9e742938770e76ed4285039",
      "19e519c330024a2ebafd5bc7f36aa55e",
      "54884c5d85eb4a6d985315d2f62bf385",
      "e07413b6bf774b5295b46a701a847981",
      "1ffbfd13c5f3499ab933a5115251cb99",
      "46df2a4d5d18493fa4dda9bcebd3727b",
      "1e465917951545a9b8e08fa73db9fce1",
      "0a0dc4f85eeb4452865bc2a91fdfb620",
      "073d5ee4e187432ca2d443f2625b2fb3",
      "723e98e2133e47b4a7f7921077c08ac0",
      "69f519df7adf4e2188b2fc9e9fea9eab",
      "6dcf8eec3e7149b89a7a31ed764d85f1",
      "6760351a95f64f11988eedb8d72f67a2",
      "b7c218a47dc646cb98620361e6049d43",
      "3b16216af79441949b912a9f93c3255a",
      "7f3e33eb76b34521b7661960e40a1e34",
      "c041d2b84b494a5d8fd15ada6bb7a8bf",
      "563ab523cab048b2a012e7fc62039125",
      "dcfc5ae5cad94907ac2b90d07519182a",
      "168d71cffac64919acff6e040e0595b3",
      "d51fd711c4c649fd818c31f38a0a54e8",
      "6207be23f45342c1a7435ef421a647a4",
      "96f0d68ffebb45ddba2bd3dec57bf7f2",
      "103242c1f7c2494e8af6a68be38ed936",
      "953b860a1ccb4b1eabe082058626422d",
      "df0b6b1f03ef4d639f4aee71cc335d06",
      "b856859510bf40c7be28462abce8f7d0",
      "d1745f095f1d478db2bd98c5c57a4bbf",
      "311ed2066d4a40e7a18ea60e8b120b65",
      "70f7eea5da2347e9b0502f4b729d3e22",
      "ad9ae33e91c846338aa06bcdccd32852",
      "86d0347432a04d7d93e5e1090a0f5ff0",
      "3bdc07ffe623449cb8d53f7887f336ad",
      "22e4c85b8263406c8a651e2a1b17eb5e",
      "e307d331484048fc9edb80bab543d2c4",
      "d725110dfc50469c89efef556a118106",
      "9cb3c1e0b6f34c5bb412baa646f9946d",
      "6cf12807d2894bd6bfa8f1c671531ea1",
      "611634eb02e1467bbc0f7509618cf93d",
      "5ee2762abb4b4874aa68d5b5313e1cde",
      "c0f28018915c43bcb436bf8f693ca6fd"
     ]
    },
    "id": "2S6xeCmO7Um4",
    "outputId": "cc9cd1ce-b095-4240-f375-4b0a95995fe8"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02ba0f5b2c78464eb883e42a8404328f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.10k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "073d5ee4e187432ca2d443f2625b2fb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "168d71cffac64919acff6e040e0595b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad9ae33e91c846338aa06bcdccd32852",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PEFT(lora config again standard can be found in the documentation)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DlTfRXtsAvLk"
   },
   "outputs": [],
   "source": [
    "\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    r=64,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"down_proj\", \"up_proj\"]\n",
    ")\n",
    "peft_model = get_peft_model(model, peft_config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training Hyperparameters\n",
    ",Can use the commented down values to get better results(requires more training time)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "id": "fPAwXj_j7vei",
    "outputId": "eb3c8f86-e005-4e4b-ea36-e01f98ce1319"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'training_arguments = TrainingArguments(\\n    output_dir=output_dir,\\n    per_device_train_batch_size=per_device_train_batch_size,\\n    gradient_accumulation_steps=gradient_accumulation_steps,\\n    optim=optim,\\n    save_steps=save_steps,\\n    logging_steps=logging_steps,\\n    learning_rate=learning_rate,\\n    fp16=True,\\n    max_grad_norm=max_grad_norm,\\n    max_steps=max_steps,\\n    warmup_ratio=warmup_ratio,\\n    group_by_length=True,\\n    lr_scheduler_type=lr_scheduler_type,\\n    push_to_hub=False\\n)'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "output_dir = \"Mistral7binstruct_lawyer\"\n",
    "per_device_train_batch_size = 2 #4\n",
    "gradient_accumulation_steps = 4\n",
    "optim = \"paged_adamw_32bit\"\n",
    "save_steps = 10\n",
    "logging_steps = 10\n",
    "learning_rate = 2e-4\n",
    "max_grad_norm = 0.3\n",
    "max_steps = 250 #100 #500\n",
    "warmup_ratio = 0.03\n",
    "lr_scheduler_type = \"cosine\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "05becd15318a48edae41d3b4968a1993",
      "ad1be08908b240389905f88f3c905a16",
      "87b319a553534bbaa0e4775acf39716d",
      "9fc5ecfcf81d40e68a1f36786212b322",
      "32ad371be95e4476ae228559a404be9c",
      "f7b62f773fb74276823dbed1939c0e42",
      "2179484f03a74addb5101418e8bd62e8",
      "8ddf3c1c961f4165a1191b6890fb86b5",
      "dd098afe014e4efcada624d322a4445f",
      "2e38649719194586866ba8ec8e7a55dc",
      "798ed68ce85b42be82cb5f7efe7b7288"
     ]
    },
    "id": "4wMmE7VGDcxH",
    "outputId": "9629b559-150c-43db-fdb4-1cccca788d7d"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05becd15318a48edae41d3b4968a1993",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/24543 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(example):\n",
    "    return tokenizer(\n",
    "        example[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",  # Or use padding=True for dynamic padding\n",
    "        max_length=256,        # You can increase this for longer inputs\n",
    "    )\n",
    "\n",
    "# Apply the tokenizer to your dataset\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "#remvoe the text column from the dataset(memory efficient)\n",
    "\n",
    "tokenized_dataset = tokenized_dataset.remove_columns([\"text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "bexCxbWmDkQ_",
    "outputId": "b5c73d9d-bce2-481c-d070-c95ec86665f9"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'training_arguments' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-14-2786240618.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtraining_arguments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove_unused_columns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'training_arguments' is not defined"
     ]
    }
   ],
   "source": [
    "training_arguments.remove_unused_columns = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "bLeS7oCmDmJ7"
   },
   "outputs": [],
   "source": [
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    optim=optim,\n",
    "    save_steps=save_steps,\n",
    "    logging_steps=logging_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    fp16=True,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    max_steps=max_steps,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    "    push_to_hub=False,\n",
    "    remove_unused_columns=False,  # ADD THIS!\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Datacollator assumess the data is already tokenized and pads the sequence dynamically and prepare tensors like input_dims,attention_mask etc**\n",
    ",mlm = masked language Model should be False as we are dealing with Mistral/falcon/gpt2 type of model which predicts the next words through the previous words not like BERT which masks random tokens in the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ivPKS86NDuH8"
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False  # MLM should be False for causal language modeling like Mistral/Falcon etc\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define your Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FtQsEbUIUoiM",
    "outputId": "90586cde-e232-497c-9c98-4b96ff2a1264"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-24-1800042336.py:2: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=training_arguments,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ndi5IRjsEHAY",
    "outputId": "b5b1182c-f81f-4ecc-d69f-932cc994e036"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
       "            function loadScript(url) {\n",
       "            return new Promise(function(resolve, reject) {\n",
       "                let newScript = document.createElement(\"script\");\n",
       "                newScript.onerror = reject;\n",
       "                newScript.onload = resolve;\n",
       "                document.body.appendChild(newScript);\n",
       "                newScript.src = url;\n",
       "            });\n",
       "            }\n",
       "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
       "            const iframe = document.createElement('iframe')\n",
       "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
       "            document.body.appendChild(iframe)\n",
       "            const handshake = new Postmate({\n",
       "                container: iframe,\n",
       "                url: 'https://wandb.ai/authorize'\n",
       "            });\n",
       "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
       "            handshake.then(function(child) {\n",
       "                child.on('authorize', data => {\n",
       "                    clearTimeout(timeout)\n",
       "                    resolve(data)\n",
       "                });\n",
       "            });\n",
       "            })\n",
       "        });\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
      "wandb: Paste an API key from your profile and hit enter:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ··········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33midealsteve001\u001b[0m (\u001b[33midealsteve001-manipal-academy-of-higher-education-dubai\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20250622_063145-19lsdlyl</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/idealsteve001-manipal-academy-of-higher-education-dubai/huggingface/runs/19lsdlyl' target=\"_blank\">Mistral7binstruct_lawyer</a></strong> to <a href='https://wandb.ai/idealsteve001-manipal-academy-of-higher-education-dubai/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/idealsteve001-manipal-academy-of-higher-education-dubai/huggingface' target=\"_blank\">https://wandb.ai/idealsteve001-manipal-academy-of-higher-education-dubai/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/idealsteve001-manipal-academy-of-higher-education-dubai/huggingface/runs/19lsdlyl' target=\"_blank\">https://wandb.ai/idealsteve001-manipal-academy-of-higher-education-dubai/huggingface/runs/19lsdlyl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [250/250 49:32, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.002500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.092700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.616900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.516800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.505600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.508100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.474600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.461000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.369300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.388000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>1.426400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.379400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>1.433800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.493900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.396000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.319800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>1.384100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>1.420000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>1.365600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.345300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>1.349300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>1.360200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>1.347200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>1.321200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.342900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=250, training_loss=1.504821559906006, metrics={'train_runtime': 3004.3876, 'train_samples_per_second': 0.666, 'train_steps_per_second': 0.083, 'total_flos': 2.2359343890432e+16, 'train_loss': 1.504821559906006, 'epoch': 0.08148631029986962})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save this model this is only the LORA adapters from the Mistral model you will need to call the Mistral Model again and then combine these two to get to our fine-tunned model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cdD6KD77fxz8",
    "outputId": "8b376d59-854d-4784-b9b3-5140955842ef"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Mistral7B_lawyer_model/tokenizer_config.json',\n",
       " 'Mistral7B_lawyer_model/special_tokens_map.json',\n",
       " 'Mistral7B_lawyer_model/chat_template.jinja',\n",
       " 'Mistral7B_lawyer_model/tokenizer.model',\n",
       " 'Mistral7B_lawyer_model/added_tokens.json',\n",
       " 'Mistral7B_lawyer_model/tokenizer.json')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_save_path = \"Mistral7B_lawyer_model\"\n",
    "trainer.save_model(model_save_path)\n",
    "tokenizer.save_pretrained(model_save_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**!!This saves teh entire Mistral+LORA adapters model(big file = 3.9GB) would not recommend to execute this code as it takes a while and eats up all the sapce and needs a lot of VRAM to load.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6ibOrtQKjYHv",
    "outputId": "1cafbcbd-da76-4cd9-b53a-9e33812a4326"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/peft/tuners/lora/bnb.py:348: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('Mistral7B_lawyer_merged/tokenizer_config.json',\n",
       " 'Mistral7B_lawyer_merged/special_tokens_map.json',\n",
       " 'Mistral7B_lawyer_merged/chat_template.jinja',\n",
       " 'Mistral7B_lawyer_merged/tokenizer.model',\n",
       " 'Mistral7B_lawyer_merged/added_tokens.json',\n",
       " 'Mistral7B_lawyer_merged/tokenizer.json')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If this is still your current trained model:\n",
    "merged_model = peft_model.merge_and_unload()\n",
    "\n",
    "# Save the merged model\n",
    "merged_model.save_pretrained(\"Mistral7B_lawyer_merged\")\n",
    "tokenizer.save_pretrained(\"Mistral7B_lawyer_merged\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optional: Remove all the cache accumulated**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "XXPvgmmVioMB"
   },
   "outputs": [],
   "source": [
    "!rm -rf wandb/\n",
    "!rm -rf Mistral7B_lawyer_model/checkpoint-*\n",
    "!rm -rf ~/.cache/huggingface/transformers\n",
    "!rm -rf ~/.cache/huggingface/datasets\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RAG Document Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 151,
     "referenced_widgets": [
      "e5094386faec43e9a81ad921b4988df5",
      "bdfa920b7fdf4e05afa034ff5ae76929",
      "6949e797fe6e43f0816ce5f0126f0d33",
      "34e0e5768adb4ae4a8596e906ceccd7e",
      "949fc2a318fd404dbf8f4895c20a67db",
      "5a744d38dc774a759da62a66b14dbab0",
      "10b5832fa21c46f8873a3b32ce539861",
      "2f15d855fc3e4b8daa68d361f25ee771",
      "b6fcdafcb01d42cab10cdef30433c8d4",
      "091453e04a1a48e898caa759a6325ce8",
      "2bbf51ee3e4642809eec8c9128c2a13f",
      "b7afc76edff9494ebad62c2968cf3211",
      "70282aa63e484eaf95fc04db478f8803",
      "99ff2a9d10d1459bb42c0643c7c23bf2",
      "d1ab6239acde4c8d8d5a605aa42135f7",
      "0f3156dc730a4cbb80b7bf5c3a762803",
      "34aec0c0cec34f0991e572348f8cfb5d",
      "c68aa8c5c34e4482a74ce56176a18b30",
      "89cb50c747c940eca61b9cfbcf62fa38",
      "03225f19b453432e8897d7ed67738ace",
      "75c61c6b9703498181b3a5f3c52df8dd",
      "25bb1290c4c24951a77382ad479715b4",
      "9cc79ba36d764338a468142de7617d06",
      "8d78d39a90fc406390cda19b359b9589",
      "714bf74dc9594c66bf98502df17ca641",
      "2728bc04e39b4ea0a0bbea120542e511",
      "eda1c082942244dbafe068a0af582b35",
      "1062cf2ac04b417ebdc834badbc3f944",
      "b1175ecaa2104f82b6ac1f12d1852266",
      "70ce579d8326478a9613fee1b28b20d8",
      "2d1bdd9c44084296a10a9ef7252cb636",
      "0a00f19566194e96a27ab4b096a4dbe7",
      "a10a2a3610384cb792c9a2712921171f"
     ]
    },
    "id": "xR18gc-tr_7Z",
    "outputId": "7aaf1233-7901-4cd7-d93d-39b9f8bd2645"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5094386faec43e9a81ad921b4988df5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7afc76edff9494ebad62c2968cf3211",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train.jsonl:   0%|          | 0.00/14.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cc79ba36d764338a468142de7617d06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/24607 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Instruction': 'What is the difference between a petition and a plaint in Indian law?', 'Response': \"A petition is a formal request submitted to a courttribunalor authority to seek a specific remedy or relief. It is commonly used for various purposessuch as filing a writ petition in the High Court or submitting a petition for divorce. On the other handa plaint is a formal written statement of a plaintiff's claim in a civil lawsuit. The key difference is that a petition is more versatile and can be used for various legal matterswhile a plaint is specific to civil cases.\"}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset = load_dataset(\"viber1/indian-law-dataset\", split=\"train\")\n",
    "print(dataset[0])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "8a376ARtsBFx"
   },
   "outputs": [],
   "source": [
    "rag_docs = [\n",
    "    f\"Q: {item['Instruction']}\\nA: {item['Response']}\"\n",
    "    for item in dataset\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 401,
     "referenced_widgets": [
      "854e188005de42199fd00f25af425d4c",
      "018308a3d8ec4d77b67025fc2dd386b3",
      "c128ec3616f549ada65af7ba48dd8000",
      "8acac537612648498cbd7a0f6c502e21",
      "8eef1e5a72a24fea80dafa35db08ecda",
      "6ebefdaa9e9f43f88a246f944602c86e",
      "8326433764cb4946abe94707c6cdef84",
      "5edf3126b2da4455973919fbabb5adac",
      "4c475c2ffda34f74ba9a709e715034c7",
      "070d3d9464c245538ebd8ab7635d059f",
      "ca79f133ee0c4a9aabed8d9eb506fbe6",
      "7f7b7a77fcbe45198343ddd102677a61",
      "d9a41388484e472d81655180f54a07ce",
      "5bda6f660c3847568ed3ce8dcf83f838",
      "7889504c5f4c452b8aed87f322bba11c",
      "97b422e8b2c9419a951bd246afa9c9f6",
      "88364af21b024d97bf07306919fe075a",
      "5af13e0d4b384ac89e075c454c1fbbe8",
      "bbd4060edfb74ae492b9385dbc0c23ff",
      "43dd7b02c1a44dbc8489c46388737465",
      "c99a286f7fb24df0b27cdde7aa803d5c",
      "40159e4bd8b148e9bade107397ef038a",
      "239cd1416c0a41669bfcf14f7964f83e",
      "47694d2b38c94c329eb820b627f7f5ca",
      "9ace70a1e28f4d97950b0ed756a8045b",
      "9d0700029e6849f988795418d328dfa6",
      "f5b9cf88e2c341a5b7587021f1668f0f",
      "b5313fa9ecc64cbfa55cb256facc456e",
      "cf86cb9c01d7414da02e30398345b4c8",
      "d247c2fd8f3f46f58cfa260b2c84c411",
      "e83fb9108c474e4fa4712bc7ddb9e58d",
      "180cf626ba1c41dc9d56c9c05ed8a3f1",
      "8b4fc6766c934a5f88b7f3ea1b902eb8",
      "a45af6de63514c75bd52a9075c7d4a4e",
      "5b0e74c72e914a2da84d2ff16ed95223",
      "6b340677d94f416a8b90596d3334fc95",
      "ea9be8417ae443828e06ccc050430949",
      "dfe6438329cf4ba1a8859902a36c095a",
      "496292c63e914682973aa065e4f7037e",
      "9714f19871fd4a3faf19f16e7ab445d7",
      "ae6ab87e20f149f5833a75e9dd3525f1",
      "278c4139bd5a4756ba3d5b0c8a82bc05",
      "2e3d2043fa0e4e3faac1bb115933d1fe",
      "b96db84c40a141598a57f294694c9876",
      "fdf8ecdf69334117a0aa55ae8f8c0e18",
      "05d7bf56c7e94b7db8faa8db117d8b40",
      "7d6732ae3146415abda908aa4c07440b",
      "4ab053b5c433450a83be388b1416fc90",
      "33e60383312e4fa496f038bc2324fd89",
      "0fa2fef61e774632a2de69c8f6d2ca0f",
      "e2bf1b0e9c354fbe97ec6737b89663aa",
      "01a1670e88dd429aa52bc661c4d9cc78",
      "a6a866455ef244edaba9fba0abca0963",
      "7b30655f19b947fe817a0b4609138f71",
      "c4b7f382821d421a80ab40765d98c0cf",
      "1bcf10d366a14fcdbebbedc03fd65beb",
      "2feb9aeaf0464fd9bcbb169874611be9",
      "42e36352e6614953838d3b09ac5dfeb1",
      "5415995b349a4c8684d0f0b587cfa646",
      "4f48525e745f43b7aa4e594f656f2e88",
      "551c10e554aa4359919927007b99f43f",
      "5593643f9129421eab65cc7a1ec97223",
      "7348a79df31146aba971ecbf559e2166",
      "ff2b9093915a4b888a42c4223f9c33f9",
      "c30d9540ed81457f98e8827ec3801d70",
      "8d865a1517d24132a2f268b4259b6298",
      "25538b00df554c2eb6256e423a0f006b",
      "d8967c30b0c94e3cbdb884bd0d23c49d",
      "306d566c32db4155b41ec9530c87e7d4",
      "133e7ea9f29e4f189777b5e852d94b51",
      "72276d7ea6844ccfae14045df9aba76d",
      "be739638340f4e4c93f892778e213fa9",
      "6a24c7f3b4414bc081134688570a4bb9",
      "4f5848fe35044562a100383dd9c45aa9",
      "e71cabf8ce7b40c2be441cc6435f9fef",
      "ad107b36f384409581849514988c1e10",
      "fd366bec95984f259fb31c9dda0c8fbb",
      "86f072ed681240c58cf2d64f44ca2ffe",
      "a71be5ae08104184926a492f82ff08bf",
      "5b4cc67f595d4f4987bbe91134d7613c",
      "617ab47926704807b717ad1888fcb3ee",
      "bc895173da5743f796c6879cac8c0d74",
      "735a5fe3c1904246a471d879a3880ce9",
      "0b7642ac32cc4a1f977e82cb58b03b7d",
      "81d0f4c189e64ebab32c0f72a2852986",
      "8cee90eaa16644ee9621d25d750dbe14",
      "ef9d27fb55b44918b9ef0029ddc6bd2f",
      "1b2a5fa62d4c4cd98f32cd91c1fe736e",
      "8c8d0057b65c4673b38c23abdea417c2",
      "a69f411b8f5d4765b338df12f289535a",
      "0da6948a06f6409b9c90d6848796c005",
      "bae38917b1724f649d89ba9d11bcd87f",
      "800de93933a948d0aa3a34f34602ca76",
      "67fdde2dde404dcfaa08e95b834b2a74",
      "0a73ed4e1cd443ecae76b1467f5112c4",
      "b4bbf506e1fc410bb8de316229e9ca26",
      "dc58df55f0204b56ba5f7093c7a11d8a",
      "66ec8eefdeef428eb284d8a5fbbee449",
      "f746a72401814af080fa16042aa80a90",
      "f4859ea91c4b4f1fbbe16fb73b84e830",
      "86be6e2342bf4c96a7926001d2603ac1",
      "1400377d7f2a4e3f8fa5562d4f5c9f3b",
      "e4e41bf7c62a4673bd54c49106971b26",
      "a2169d309d8a4182ba366b1029a4b6e5",
      "2088287d2a384f8a9b8431e95bc0269a",
      "f4b99cf92d6642d2a8038d8589cce503",
      "f893d8330cb643928b77a71975a2031b",
      "277df438b68f4a40a18eb20715115f96",
      "26f26d45e8454ca8aac3fa8355adc914",
      "c8d0866164304212901e71b750826721",
      "051dd1b77ed146999e6e5d0d7adfec35",
      "ce32e02ef8f24225b2eb161d5c8eb2dd",
      "b040fceb96aa4413b2eb1b1ff6a988a2",
      "81c961faff0a4b358ef4efb8041e81cc",
      "d87ae91e14d04488ba82f7365249b99a",
      "f31374078131415695eec42d3625054d",
      "27008b46bcf7490bab6afeabeb94726a",
      "5d5c441919f745d2b50541232e569bfc",
      "65fd93a205c64a47abf4f6d70a71c490",
      "d24ece9eff974c5796b58bb988e7d9f7",
      "a846d2eca459469390fe5747d5688a7b",
      "6a89741a04d34ecba14645a9634193d0",
      "8ed00b927ba945aab4f433244c9b5176",
      "25df9a530212462084e979c8ab174e25",
      "28b7618eb89e43a1a09c1c461b93b64c",
      "48e58db2a938455bb3e0b56a80911551",
      "a72d14c9c480483e92afb75ec991a138",
      "ece949d371b8495c95e6fed353b21d0f",
      "1c984b09da98486c97402ea89f603fb3",
      "a7f2fbe38bb347e9b2f1efe5d6edcaee",
      "55d7e1f2991343c99adb7284c9c3b48a",
      "e6d4a35f750f43a6a494cc32bd0aec81"
     ]
    },
    "id": "27MIdxvCsgO0",
    "outputId": "eaf96430-cece-4cb6-b0ea-165a133bfbb4"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "854e188005de42199fd00f25af425d4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f7b7a77fcbe45198343ddd102677a61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "239cd1416c0a41669bfcf14f7964f83e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a45af6de63514c75bd52a9075c7d4a4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdf8ecdf69334117a0aa55ae8f8c0e18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bcf10d366a14fcdbebbedc03fd65beb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25538b00df554c2eb6256e423a0f006b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86f072ed681240c58cf2d64f44ca2ffe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c8d0057b65c4673b38c23abdea417c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4859ea91c4b4f1fbbe16fb73b84e830",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "051dd1b77ed146999e6e5d0d7adfec35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a89741a04d34ecba14645a9634193d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/769 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embed_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "embeddings = embed_model.encode(rag_docs, convert_to_numpy=True, show_progress_bar=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inference**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Mistral7B_lawyer_model\", trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Optional for safety\n",
    "\n",
    "# Quantization config for Mistral (must match training)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "# Load base Mistral model in 4-bit\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"mistralai/Mistral-7B-Instruct-v0.1\",\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Load LoRA adapter on top\n",
    "model = PeftModel.from_pretrained(base_model, \"Mistral7B_lawyer_model\")\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RAG Pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pKhMZMAQuDE3"
   },
   "outputs": [],
   "source": [
    "\n",
    "# === CONFIG ===\n",
    "model_path = \"/content/Mistral7B_lawyer_merged\"  # or mistral + LoRA if needed\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    offload_folder=\"offload\"\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "# === RAG EMBEDDING SETUP ===\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# rag_docs = [f\"Q: {item['Instruction']}\\nA: {item['Response']}\" for item in dataset]\n",
    "# embeddings = embed_model.encode(rag_docs, convert_to_numpy=True)\n",
    "\n",
    "index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "index.add(embeddings)\n",
    "\n",
    "def retrieve_context(query, top_k=3):\n",
    "    query_vec = embed_model.encode([query])\n",
    "    D, I = index.search(query_vec, top_k)\n",
    "    return [rag_docs[i] for i in I[0]]\n",
    "\n",
    "def rag_chat(query):\n",
    "    context = retrieve_context(query)\n",
    "    prompt = (\n",
    "        \"<s>[INST] You are a legal assistant. Use the following context to answer the query.\\n\\n\"\n",
    "        + \"\\n\\n\".join(context) +\n",
    "        f\"\\n\\nQuestion: {query}\\nAnswer: [/INST]\"\n",
    "    )\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=256,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            repetition_penalty=1.1,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True).split(\"[/INST]\")[-1].strip()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ChatBot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XI0xcsFgujRT",
    "outputId": "72ed9eaa-f851-4d5c-971a-11e1e0a00559"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧑‍⚖️ LawBot RAG Chat (type 'exit' to quit)\n",
      "\n",
      "You: hey\n",
      "Thinking...\n",
      "\n",
      "📜 LawBot: Hello! How can I assist you today?\n",
      "\n",
      "You: what are you??\n",
      "Thinking...\n",
      "\n",
      "📜 LawBot: I am an AI language model and a legal assistant.\n",
      "\n",
      "You: an a magistrate take cognizance on a police report without sanction under Sec 197 CrPC?\n",
      "Thinking...\n",
      "\n",
      "📜 LawBot: No, a magistrate cannot take cognizance of a police report without sanction under Section 197 of the CrPC. Section 197 provides that no court or tribunal shall take cognizance of any offence except upon a police report filed by an officer in charge of a police station, or upon an order from a competent authority. Therefore, before a magistrate can take cognizance of a police report, it must first be filed by an officer in charge of a police station and approved by a competent authority.\n",
      "\n",
      "You: Explain what happened in Maneka Gandhi v. Union of India\n",
      "Thinking...\n",
      "\n",
      "📜 LawBot: In Maneka Gandhi v. Union of India, the Supreme Court of India considered whether the government's decision to detain Mrs. Gandhi under the Maintenance of Internal Security Act, 1971 was valid. The court held that the detention order was not based on any evidence and therefore was not justified under Section 304(2) of the Prevention of Insurgency Act, 1971. The court also observed that the government had failed to provide any reasonable justification for the detention, which violated Mrs. Gandhi's fundamental right to liberty under Article 21 of the Indian Constitution.\n",
      "\n",
      "The court further held that while the power of detention was available under Section 304(1) of the Prevention of Insurgency Act, 1971, it could only be exercised after giving the person concerned an opportunity to make representations before the competent authority. The court also found that the government had not complied with the procedural requirements set out in Section 304(1). Therefore, the court declared the detention order unconstitutional and ordered Mrs. Gandhi's immediate release.\n",
      "\n",
      "You: exit\n",
      "👋 Exiting LawBot. Goodbye!\n"
     ]
    }
   ],
   "source": [
    "def wait_for_keypress():\n",
    "    #Detects single keypress like Esc\n",
    "    fd = sys.stdin.fileno()\n",
    "    old_settings = termios.tcgetattr(fd)\n",
    "    try:\n",
    "        tty.setraw(fd)\n",
    "        key = sys.stdin.read(1)\n",
    "    finally:\n",
    "        termios.tcsetattr(fd, termios.TCSADRAIN, old_settings)\n",
    "    return key\n",
    "\n",
    "def run_chat():\n",
    "    print(\"🧑‍⚖️ LawBot RAG Chat (type 'exit' to quit)\\n\")\n",
    "    while True:\n",
    "        user_input = input(\"You: \").strip()\n",
    "        if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "            print(\"👋 Exiting LawBot. Goodbye!\")\n",
    "            break\n",
    "\n",
    "        print(\"Thinking...\\n\")\n",
    "        try:\n",
    "            response = rag_chat(user_input)\n",
    "            print(f\"📜 LawBot: {response}\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {str(e)}\\n\")\n",
    "\n",
    "\n",
    "run_chat()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bDUMkwazcF43"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7715587,
     "sourceId": 12245358,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7721727,
     "sourceId": 12254678,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
